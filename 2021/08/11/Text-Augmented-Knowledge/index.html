<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"lsx-sneakerprogrammer.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="前言《Text-Augmented Knowledge Representation Learning Based on Convolutional Network》阅读笔记 Introduction NotesKnowledge Graphs have the relationship triples (subject, relation, object) denoted as (h,r,t),">
<meta property="og:type" content="article">
<meta property="og:title" content="Text-Augmented-Knowledge">
<meta property="og:url" content="https://lsx-sneakerprogrammer.github.io/2021/08/11/Text-Augmented-Knowledge/index.html">
<meta property="og:site_name" content="永缘空的博客">
<meta property="og:description" content="前言《Text-Augmented Knowledge Representation Learning Based on Convolutional Network》阅读笔记 Introduction NotesKnowledge Graphs have the relationship triples (subject, relation, object) denoted as (h,r,t),">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-08-10T16:21:44.000Z">
<meta property="article:modified_time" content="2021-08-11T15:18:54.052Z">
<meta property="article:author" content="永缘空">
<meta property="article:tag" content="knowledgeGraph">
<meta property="article:tag" content="paper">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://lsx-sneakerprogrammer.github.io/2021/08/11/Text-Augmented-Knowledge/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Text-Augmented-Knowledge | 永缘空的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>


<script type="text/javascript" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>


<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">永缘空的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">学习生活记录</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签<span class="badge">12</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类<span class="badge">7</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档<span class="badge">17</span></a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lsx-sneakerprogrammer.github.io/2021/08/11/Text-Augmented-Knowledge/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.jpg">
      <meta itemprop="name" content="永缘空">
      <meta itemprop="description" content="山东大学17级计算机科学与技术，澳大利亚国立大学Bachelor of Advanced Computing(Honors), 澳大利亚国立大学Master of Machine Learning and Computer Vision主要学习NLP, Machine Learning等机器学习领域，此为个人博客。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="永缘空的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Text-Augmented-Knowledge
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-08-11 00:21:44 / 修改时间：23:18:54" itemprop="dateCreated datePublished" datetime="2021-08-11T00:21:44+08:00">2021-08-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Knowledge-Graph/" itemprop="url" rel="index"><span itemprop="name">Knowledge Graph</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/08/11/Text-Augmented-Knowledge/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/08/11/Text-Augmented-Knowledge/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>7.7k</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>《Text-Augmented Knowledge Representation Learning Based on Convolutional Network》阅读笔记</p>
<h1 id="Introduction-Notes"><a href="#Introduction-Notes" class="headerlink" title="Introduction Notes"></a>Introduction Notes</h1><p>Knowledge Graphs have the relationship triples (subject, relation, object) denoted as (h,r,t), which are useful for IR tasks.</p>
<h2 id="KGs-completion-or-link-prediction"><a href="#KGs-completion-or-link-prediction" class="headerlink" title="KGs completion or link prediction"></a>KGs completion or link prediction</h2><p>Some research focused on the knowledge graph completion or link prediction task which aims to predict missing triples in KGs.</p>
<ul>
<li>$\text{Rescal model}$: Matrix decomposition for knowledge representation learning.</li>
<li>$\text{DistMult model}$: Set relation matrix as diagonal matrix; Use “circular correlation” of the head and tail entity vectors to represent the entity pair.</li>
<li>$\text{ConvE model}$: The first model applying convolutional neural network for the knowledge graph completion task.</li>
<li>$\text{ConvKB model}$: Apply a convolutional neural network to calculate the fraction of the input triples.<a id="more"></a>
<h2 id="Importance-of-Text-Descriptions"><a href="#Importance-of-Text-Descriptions" class="headerlink" title="Importance of Text Descriptions"></a>Importance of Text Descriptions</h2>The semantic expression between entities can identify true triples.<br>For example: </li>
<li>the triple (Barack Hussein Obama Sr, parent of, Barack Obama)</li>
<li>Indicates “Barack Hussein Obama Sr” is the parent of “Barack Obama”.</li>
<li>We don’t know “parent of” refers to “father” or “mother”. </li>
</ul>
<p>The text description helps confirm the true entity.</p>
<ul>
<li>In the text description of the true entity, the keywords are “he” and “his wife”.</li>
<li>The false entity text description has “she” and “mother”. </li>
<li>Description information can improve task performance such as link prediction.</li>
</ul>
<h2 id="TA-ConvKB-Model"><a href="#TA-ConvKB-Model" class="headerlink" title="TA-ConvKB Model"></a>TA-ConvKB Model</h2><p>Because ConvKB model only learn triples from KG and does not utilize description information. The author raised a text augmented method based on ConvKB (TA-ConvKB). <span style="border-bottom:2px dashed black;">It combines the information of factual triples and descriptions to enhance the accuracy of knowledge graphs link prediction.</span></p>
<p>$\textbf{Steps}$</p>
<ol>
<li>Pretrain the entities descriptions and extract a multiple of keywords.</li>
<li>Use fastText to encode those words into vectors.</li>
<li>Input word embeddings into the A-BiLSTM encoder to extract distinctive features.</li>
<li>Acquire a text vector that best expresses the entity information</li>
<li>Use the gate mechanism (inspired by LSTM gate units) to integrate structural embeddings obtained by StransE model and textual embeddings.</li>
</ol>
<h2 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h2><ol>
<li>Introduce the TA-ConvKB model and use A-BiLSTM to encode entities descriptions.</li>
<li>Use a brand-new gate mechanism to combine two types of embeddings</li>
<li>Get better results than ConvKB Model when analyzing FB15k-237 and WN18RR data bases in link prediction. Combine other models such as TransH can also get good result.</li>
</ol>
<h1 id="Related-Work-Notes"><a href="#Related-Work-Notes" class="headerlink" title="Related Work Notes"></a>Related Work Notes</h1><h2 id="TransE-Model"><a href="#TransE-Model" class="headerlink" title="TransE Model"></a>TransE Model</h2><p>The TransE model regard relations as changing from the heads to tails on the same low-dimensional plane. The score function is:</p>
<script type="math/tex; mode=display">E(h, r, t) = ||h + r - t||_{L_{n}}</script><p>It uses $L_{n}$ distance to measure between the converted head entity $h + r$ and tail entity $t$</p>
<h2 id="ConvKB-Model"><a href="#ConvKB-Model" class="headerlink" title="ConvKB Model"></a>ConvKB Model</h2><p>ConvKB uses a convolution neural network to capture the overall relations and transformation features of entities and relations in the knowledge base.</p>
<p>$\textbf{Representation of triples}$<br>Each triple (head, relation, tail) is represented as a three-column matrix.</p>
<h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><p>Introduce some methods in order to enhance the representation learning of KG, utilizing text information in knowledge base. In general, text augmented embedding models achieve state-of-the-art performance through integrating knowledge and text.</p>
<h1 id="Text-Augmented-Knowledge-Graph-Representation-Notes"><a href="#Text-Augmented-Knowledge-Graph-Representation-Notes" class="headerlink" title="Text-Augmented Knowledge Graph Representation Notes"></a>Text-Augmented Knowledge Graph Representation Notes</h1><p>Define a knowledge graph as </p>
<script type="math/tex; mode=display">G = (E, R, T)</script><p>where E is the set of entities, R is the set of relation types, and T is the set of factual triples.</p>
<p>For each knowlege triple $(h, r, t) \in T$, we have</p>
<script type="math/tex; mode=display">r \in R \space \space \text{ and } \space \space h, t \in E</script><p>$\textbf{two kinds of entity representation types}$:</p>
<ul>
<li>structure representation $e_{s}$</li>
<li>text representation $e_{d}$</li>
</ul>
<p>For a given knowledge triple $(h, r, t) \in T$,</p>
<ul>
<li>$h<em>{s} \in e</em>{s}$ represents head structure representation</li>
<li>$t<em>{s} \in e</em>{s}$ represents tail structure representation</li>
<li>$h<em>{d} \in e</em>{d}$ represents the description text coding of the head</li>
<li>$t<em>{d} \in e</em>{d}$ represents the description text coding of the tail</li>
<li>$R_{d}$ represents the corresponding low-dimensional vectors of entities, relations, and descriptions.</li>
<li>For example, embedding $h, t \in E$ and $r \in R$ are equal<br>to $h, t, r \in R_{d}$ respectively in the d−dimension.</li>
</ul>
<h2 id="Neural-Network-Text-Encoding"><a href="#Neural-Network-Text-Encoding" class="headerlink" title="Neural Network Text Encoding"></a>Neural Network Text Encoding</h2><p>$\textbf{Preprocess}$:</p>
<ul>
<li>Remove all stop words </li>
<li>Mark all the words(phrases which entity names in the training set) in the descriptions</li>
<li>Extract multi-themed words for each entity as description</li>
<li>Use fastText to encode the words into word vectors as input to the A-BiLSTM encoder</li>
</ul>
<p>$\textbf{BiLSTM Encoder}$:<br>To construct BiLSTM, the author need to apply a forward and backward LSTM network to each training sequence and connnect them to the same output layer. <span style="border-bottom:2px dashed black;">It can provide the complete contextual information of each sequence point to the output layer.</span></p>
<p>$\textbf{Self Attentive BiLSTM Encoder}$:<br>The author lead an attention mechanism into BiLSTM to encode depending on the different relations of the context in order to improve the text representations. </p>
<p>The author uses the attention mechanism to extract the features from $h_{i}$ embeddings.</p>
<p>For each position i of the text description, the attention to a given word embedding $h<em>{i}$ is defined as $a</em>{i}$.</p>
<script type="math/tex; mode=display">a_{i} = \frac{exp(v_{a}^{T}tanh(W_{a}h_{i} + b_{a}))}{\sum_j exp(v_{a}^{T}tanh(W_{a}h_{j} + b_{a}))}</script><p>where $h<em>{i} \in  R</em>{d}$ is the output of BiLSTM at position $i, v<em>{a} \in R^{d \times d}$ is a parameter vector, $W</em>{a}, b<em>{a} \in R</em>{d}$ are parameter matrixs.</p>
<p>Therefore, use the attention mechanism, we could know which part of the embeddings will be concerned. The final description embedding will be $e<em>{i} = \sum</em>{i=1}^{n} a<em>{i} \cdot h</em>{i}$</p>
<h2 id="Joint-Structure-and-Text-Encoding"><a href="#Joint-Structure-and-Text-Encoding" class="headerlink" title="Joint Structure and Text Encoding"></a>Joint Structure and Text Encoding</h2><p>To balance two sources of information, the author combines the $e<em>{s}$ and $e</em>{d}$ for an entity $e$.</p>
<h3 id="Linear-Interaction"><a href="#Linear-Interaction" class="headerlink" title="Linear Interaction"></a>Linear Interaction</h3><script type="math/tex; mode=display">e = \alpha \cdot e_{s} + (1 - \alpha) \cdot e_{d}</script><p>where $0 &lt; \alpha &lt; 1$ and $\alpha$ is uncertain.</p>
<h3 id="Second-order-Interaction"><a href="#Second-order-Interaction" class="headerlink" title="Second-order Interaction"></a>Second-order Interaction</h3><p>Referring to the gated mechanism in LSTM gate units:</p>
<script type="math/tex; mode=display">e = tanh(e_{s}) \otimes sigmoid(e_{d})</script><p>where $\otimes$ is point multiplication</p>
<h3 id="Triple-Embedding"><a href="#Triple-Embedding" class="headerlink" title="Triple Embedding"></a>Triple Embedding</h3><p>$e$ is the final entity representation. Because there is no description for relations, the structural embeddings are regarded as the textual embeddings for relation. <span style="border-bottom:2px dashed black;">The triple embeddings containing entity embeddings and relation embeddings are fed into CNN to extract features for link prediction.</span></p>
<h2 id="Training-and-Socre-Function"><a href="#Training-and-Socre-Function" class="headerlink" title="Training and Socre Function"></a>Training and Socre Function</h2><ul>
<li>$\kappa$ denotes the set of filters</li>
<li>$\tau$ denotes the number of filters</li>
<li>$\tau = |\kappa|$ results in $\tau$ feature maps</li>
<li>Concatenate the $\tau$ feature maps into a vector $\in R^{\tau d \times 1}$</li>
<li>For the triple $(h, r, t)$, the score function of TA-ConvKB is as follows:<script type="math/tex; mode=display">f(h, r, t) = concat(g(tanh([h_s, r, t_s]) \otimes sigmoid([h_d, r, t_d])) * \kappa) \cdot w</script>where $w \in R^{\tau d \times 1}$, $g$ is Relu activation functions, $*$ denotes a convolution operator.</li>
<li>Use the Adam optimizer and loss function L<script type="math/tex; mode=display">L = \sum (h, r, t) log (1 + exp((l_{(h, r, t)}) \cdot f(h, r, t))) + \frac{\lambda}{2} ||w||_{2}^{2}</script>where $(h, r, t) \in G ∪ G^{‘}$, $l(h,r,t) = 1$ when $(h, r, t) \in G$, $l(h,r,t) = −1$ when $(h, r, t) \in G$, here $G^{‘}$ is a collection of invalid triples generated by corrupting valid triples in $G$.</li>
</ul>
<h1 id="Experiments-Notes"><a href="#Experiments-Notes" class="headerlink" title="Experiments Notes"></a>Experiments Notes</h1><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>WN18RR and FB15k237.</p>
<h2 id="Assessment-Strategy"><a href="#Assessment-Strategy" class="headerlink" title="Assessment Strategy"></a>Assessment Strategy</h2><p>$\textbf{KG link prediction task}$: the purpose is to predict a missing entity given a relation and another entity. The author ranks the scores of score function on test triples to get the result. To construct corrupted triples, replace one of the two entities in the triples. </p>
<p>$\textbf{Have problems to ask}$<br><span style="border-bottom:2px dashed black;">“We use the “Filtered” setting protocol, not taking any corrupted triples that appear in the KG into accounts.”</span></p>
<p>$\textbf{Three metrics}$:</p>
<ul>
<li>mean rank (MR) (Need lower MR)</li>
<li>mean reciprocal rank (MRR) (Need higher MRR)</li>
<li>H@10 (i.e., the proportion of the valid test triples ranking in top 10 predictions) (higher H@10.)</li>
</ul>
<h2 id="Training-Strategy"><a href="#Training-Strategy" class="headerlink" title="Training Strategy"></a>Training Strategy</h2><h3 id="Text-Embeddings"><a href="#Text-Embeddings" class="headerlink" title="Text Embeddings"></a>Text Embeddings</h3><p>$\textbf{In FB15k-237}$  </p>
<ul>
<li>10 keywords are extracted for each entity description</li>
<li>FastText is used to encode a 100-dimensional word vector</li>
<li>In the A-BiLSTM encoder, embedding dimension is 100, the sequence length is 10 and the number of hidden layer units is 50</li>
<li>Finally, output 100-d vector  </li>
</ul>
<p>$\textbf{In WN18RR}$  </p>
<ul>
<li>Similar with FB15k-237 except the keywords is 3</li>
</ul>
<h3 id="Structural-Embeddings"><a href="#Structural-Embeddings" class="headerlink" title="Structural Embeddings"></a>Structural Embeddings</h3><ul>
<li>Train STransE for 3,000 epochs </li>
</ul>
<h3 id="Hyperparameters"><a href="#Hyperparameters" class="headerlink" title="Hyperparameters"></a>Hyperparameters</h3><ul>
<li>200 epochs</li>
<li>Adam as optimizer</li>
<li>ReLU as the activation function</li>
<li>Batch size at 256</li>
<li>L2-regularizer $\lambda$ at 0.005</li>
<li>In WN18RR, using k = 100, τ = 200, the truncated normal distribution for filter $w$ initialization, initial learning rate at 1e−4</li>
<li>In FB15k-237, k = 100, τ = 40, [0.1, 0.1, −0.1] for filter $w$ initialization, and the initial learning rate at 5e−6.</li>
</ul>
<h2 id="Main-Experimental-Results"><a href="#Main-Experimental-Results" class="headerlink" title="Main Experimental Results"></a>Main Experimental Results</h2><p>In both dataset, the H@10 evaluation method gets the highest performance compared with other models.</p>
<h1 id="Future-Work-Notes"><a href="#Future-Work-Notes" class="headerlink" title="Future Work Notes"></a>Future Work Notes</h1><ul>
<li>Use more accurate word vector representations, such as Elmo model</li>
<li>Explore kinds of combinations between structural embeddings and textual embeddings</li>
<li>Integrate the image information of the entities</li>
</ul>

    </div>

    
    
    

    <div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>

      
    </div>

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/knowledgeGraph/" rel="tag"># knowledgeGraph</a>
              <a href="/tags/paper/" rel="tag"># paper</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/08/08/Knowledge-Graphs-cookbook-1/" rel="prev" title="Knowledge Graphs cookbook (1)">
      <i class="fa fa-chevron-left"></i> Knowledge Graphs cookbook (1)
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/08/16/Representing-Text-for-Joint-Embedding-of-Text-and-Knowledge-Bases-%E7%AC%94%E8%AE%B0/" rel="next" title="Representing Text for Joint Embedding of Text and Knowledge Bases 笔记">
      Representing Text for Joint Embedding of Text and Knowledge Bases 笔记 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction-Notes"><span class="nav-number">2.</span> <span class="nav-text">Introduction Notes</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#KGs-completion-or-link-prediction"><span class="nav-number">2.1.</span> <span class="nav-text">KGs completion or link prediction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Importance-of-Text-Descriptions"><span class="nav-number">2.2.</span> <span class="nav-text">Importance of Text Descriptions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TA-ConvKB-Model"><span class="nav-number">2.3.</span> <span class="nav-text">TA-ConvKB Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Contributions"><span class="nav-number">2.4.</span> <span class="nav-text">Contributions</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Related-Work-Notes"><span class="nav-number">3.</span> <span class="nav-text">Related Work Notes</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#TransE-Model"><span class="nav-number">3.1.</span> <span class="nav-text">TransE Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ConvKB-Model"><span class="nav-number">3.2.</span> <span class="nav-text">ConvKB Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Others"><span class="nav-number">3.3.</span> <span class="nav-text">Others</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Text-Augmented-Knowledge-Graph-Representation-Notes"><span class="nav-number">4.</span> <span class="nav-text">Text-Augmented Knowledge Graph Representation Notes</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-Network-Text-Encoding"><span class="nav-number">4.1.</span> <span class="nav-text">Neural Network Text Encoding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Joint-Structure-and-Text-Encoding"><span class="nav-number">4.2.</span> <span class="nav-text">Joint Structure and Text Encoding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-Interaction"><span class="nav-number">4.2.1.</span> <span class="nav-text">Linear Interaction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Second-order-Interaction"><span class="nav-number">4.2.2.</span> <span class="nav-text">Second-order Interaction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Triple-Embedding"><span class="nav-number">4.2.3.</span> <span class="nav-text">Triple Embedding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Training-and-Socre-Function"><span class="nav-number">4.3.</span> <span class="nav-text">Training and Socre Function</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Experiments-Notes"><span class="nav-number">5.</span> <span class="nav-text">Experiments Notes</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Dataset"><span class="nav-number">5.1.</span> <span class="nav-text">Dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Assessment-Strategy"><span class="nav-number">5.2.</span> <span class="nav-text">Assessment Strategy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Training-Strategy"><span class="nav-number">5.3.</span> <span class="nav-text">Training Strategy</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Text-Embeddings"><span class="nav-number">5.3.1.</span> <span class="nav-text">Text Embeddings</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Structural-Embeddings"><span class="nav-number">5.3.2.</span> <span class="nav-text">Structural Embeddings</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hyperparameters"><span class="nav-number">5.3.3.</span> <span class="nav-text">Hyperparameters</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Main-Experimental-Results"><span class="nav-number">5.4.</span> <span class="nav-text">Main Experimental Results</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Future-Work-Notes"><span class="nav-number">6.</span> <span class="nav-text">Future Work Notes</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="永缘空"
      src="/images/header.jpg">
  <p class="site-author-name" itemprop="name">永缘空</p>
  <div class="site-description" itemprop="description">山东大学17级计算机科学与技术，澳大利亚国立大学Bachelor of Advanced Computing(Honors), 澳大利亚国立大学Master of Machine Learning and Computer Vision主要学习NLP, Machine Learning等机器学习领域，此为个人博客。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2020 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">永缘空</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">57k</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,0' opacity='0.5' zIndex='-1' count='150' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : true,
      appId      : 'mDMP1YcO9YbOcBLneAjQaJuj-gzGzoHsz',
      appKey     : 'h9IkJ3I6bmAT2UG64PFMm7KI',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

  <!-- 页面点击小红心 -->
  <script type="text/javascript" src="/js/src/clicklove.js"></script>

</body>
</html>

<!-- 页面点击小红心 -->
  <script type="text/javascript" src="/js/src/clicklove.js"></script>
